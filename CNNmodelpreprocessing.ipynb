{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqAZTkX667hEqwy4mWFsvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuminosukeSato/minddrone/blob/main/CNNmodelpreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sm95cQX8jlJ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tqdm\n",
        "from torch.autograd import Variable\n",
        "def preprocessing(floatArray):\n",
        "    window = np.hamming(125)\n",
        "    w = np.empty(125)\n",
        "    d2 = np.empty((floatArray.shape[0],27*16))\n",
        "    for i in range(floatArray.shape[0]-125):\n",
        "        for j in range(15):\n",
        "            w = np.abs(np.fft.fftn(floatArray[i:i+125,j]*window))\n",
        "            d2[i,27*j:27*j+26] = np.log10(1 + w[4:30])\n",
        "    return d2\n",
        "def labeler(array):\n",
        "  label = np.empty(175)\n",
        "  for i in range(175):\n",
        "    if i <= 25:\n",
        "      label[i] = 0 #0 is normal\n",
        "    elif i <= 50:\n",
        "      label[i] = 1 #1is forward\n",
        "    elif i <= 75:\n",
        "      label[i] = 2 # 2 is righet\n",
        "    elif i <= 100:\n",
        "      label[i] = 3\n",
        "    elif i <= 125:\n",
        "      label[i] = 4\n",
        "    elif i <= 150:\n",
        "      label[i] = 5\n",
        "    else:\n",
        "      label[i] = 6\n",
        "  return label\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = np.loadtxt(\"/content/OpenBCI-RAW-2022-02-16_16-33-39.txt\",dtype='str',delimiter=\",\",skiprows=5)\n",
        "    b = data[:,1:17]\n",
        "    floatArray = b.astype(float)\n",
        "    d2 = np.empty((floatArray.shape[0],27*16))\n",
        "    d2 = preprocessing(floatArray)\n",
        "    ts = d2[125*60-1:125*130-1,:]\n",
        "    ts=torch.Tensor(ts)\n",
        "    ts = ts.view(175,50,432)\n",
        "    label = labeler(ts)\n",
        "    train_data, test_data, train_label, test_label = train_test_split(ts, label, test_size=0.2,shuffle = False)\n",
        "    train_x = torch.Tensor(train_data)\n",
        "    test_x = torch.Tensor(test_data)\n",
        "    #train_y = torch.LongTensor(train_label)  # torch.int64のデータ型に\n",
        "    #test_y = torch.LongTensor(test_label)\n",
        "    train_y = torch.Tensor(train_label)\n",
        "    test_y = torch.Tensor(test_label)\n",
        "    train_dataset = TensorDataset(train_x, train_y)\n",
        "    test_dataset = TensorDataset(test_x, test_y)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset,batch_size=128,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oOJ0Pj1o65h"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 60,kernel_size=(1, 15),stride=(1,3))\n",
        "        self.conv2 = nn.Conv2d(60, 60, kernel_size=(1, 4),stride=(1,2))\n",
        "        self.conv3 = nn.Conv2d(60, 60, kernel_size=(1, 17),stride=(1,3))\n",
        "        self.conv4 = nn.Conv2d(60, 90, kernel_size=(1, 3),stride=(1,1))\n",
        "        self.conv5 = nn.Conv2d(90, 120, kernel_size=(1, 1),stride=(1,1))\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(1, 2),stride=(1,2))\n",
        "        self.soft = nn.Softmax()\n",
        "        self.li=nn.Linear(7,184320)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.pool(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.pool(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.pool(x)\n",
        "      x = self.conv5(x)\n",
        "      #x = self.li(x)\n",
        "      #x = self.soft(x)\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "x = torch.ones(128,1,21,432)\n",
        "o = net(x)\n",
        "print(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "CxjxMtCmvu9S",
        "outputId": "36334d02-c18e-473c-f1c1-9cdafc5cbae4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-48faa7ca0753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m432\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b9833a2e444a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (21 x 1). Kernel size: (1 x 3). Kernel size can't be greater than actual input size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "\n",
        "# loss関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化関数の定義\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "### トレーニング ###\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # データ[input, labels]を取得\n",
        "        inputs, labels = data\n",
        "        print(inputs.shape)\n",
        "        # 均配の初期化\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.view(128,1,50,432)\n",
        "        print(inputs.shape)\n",
        "        #フォワード処理forward関数の実行）\n",
        "        outputs = net(inputs)\n",
        "        # lossを計算\n",
        "        loss = criterion(outputs, labels)\n",
        "        print(loss)\n",
        "        # lossから均配を計算\n",
        "        loss.backward()\n",
        "\n",
        "        # 最適化(均配からパラメータの更新)\n",
        "        optimizer.step()\n",
        "\n",
        "        # 学習過程を表示\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % \n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# 次回から学習しなくていいように、学習済みモデルのパラメータを\"net_00.prmとして保存\"\n",
        "params = net.state_dict()\n",
        "torch.save(params, \"net_00.prm\", pickle_protocol=4)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "### 学習済みモデルのテスト ###\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad(): # パラメータ保持の停止（test時には必要ないので。）\n",
        "    for data in val_loader:\n",
        "        signal, labels = data\n",
        "        signal = torch.unsqueeze(signal,1)\n",
        "        signal = torch.unsqueeze(signal,1)\n",
        "\n",
        "        # 各ラベルの確率がoutputに出力される\n",
        "        outputs = net(signal)\n",
        "\n",
        "        # torch.maxは最大値とそのインデックス(ラベル)を返す\n",
        "        # 最大値は無視してラベルのみを保存\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# 正解率を出力\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' %\n",
        "        (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "Rb_aZDJkOYUK",
        "outputId": "16ca48bd-bfde-4a17-c198-0d4916348513"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 50, 432])\n",
            "torch.Size([128, 1, 50, 432])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dc85b5f5d3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# lossを計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# lossから均配を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad(): # パラメータ保持の停止（test時には必要ないので。）\n",
        "    for data in val_loader:\n",
        "        signal, labels = data\n",
        "        signal = torch.unsqueeze(signal,1)\n",
        "        signal = torch.unsqueeze(signal,1)\n",
        "        # 各ラベルの確率がoutputに出力される\n",
        "        outputs = net(signal)\n",
        "\n",
        "        # torch.maxは最大値とそのインデックス(ラベル)を返す\n",
        "        # 最大値は無視してラベルのみを保存\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# 正解率を出力\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' %\n",
        "        (100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fwfhszIzukGG",
        "outputId": "fcb92259-f95e-4cee-c023-8ce7320c713a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 73,  73, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[ 73, 109, 109,  40, 109,  73]],\n",
            "\n",
            "        [[ 73, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 73, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[ 52, 109,  73,  52,  73,  89]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 119, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109,  40, 109,  79]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 119, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109, 119]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109,  52,  73,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 109, 109,  89,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109, 109]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109, 119]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109, 109]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109,  40, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109,  40, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109,  73, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  79]],\n",
            "\n",
            "        [[ 52,  52,  52,  52,  52,  52]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109,  17, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109, 109, 109, 109,  73]],\n",
            "\n",
            "        [[109, 109,  73, 109, 109,  73]]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-aa77f17097cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 正解率を出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (128) at non-singleton dimension 2"
          ]
        }
      ]
    }
  ]
}