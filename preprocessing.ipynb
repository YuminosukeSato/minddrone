{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing",
      "provenance": [],
      "authorship_tag": "ABX9TyMupLG+P6Ui1dNmCePyMCwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuminosukeSato/minddrone/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm95cQX8jlJ0",
        "outputId": "e6f1724a-947f-439b-82e0-8a4452a71557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6. 3. 6. ... 5. 6. 3.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "def preprocessing(floatArray):\n",
        "    window = np.hamming(125)\n",
        "    w = np.empty(125)\n",
        "    d2 = np.empty((floatArray.shape[0],27*16))\n",
        "    for i in range(floatArray.shape[0]-125):\n",
        "        for j in range(15):\n",
        "            w = np.abs(np.fft.fftn(floatArray[i:i+125,j]*window))\n",
        "            d2[i,27*j:27*j+26] = np.log10(1 + w[4:30])\n",
        "    return d2\n",
        "def labeler(array):\n",
        "  label = np.empty(array.shape[0])\n",
        "  for i in range(array.shape[0]-125):\n",
        "    if i <= 1250:\n",
        "      label[i] = 0 #0 is normal\n",
        "    elif i <= 2500:\n",
        "      label[i] = 1 #1is forward\n",
        "    elif i <= 3750:\n",
        "      label[i] = 2 # 2 is righet\n",
        "    elif i <= 5000:\n",
        "      label[i] = 3\n",
        "    elif i <= 6250:\n",
        "      label[i] = 4\n",
        "    elif i <= 7500:\n",
        "      label[i] = 5\n",
        "    else:\n",
        "      label[i] = 6\n",
        "  return label\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = np.loadtxt(\"/content/OpenBCI-RAW-2022-02-16_16-37-06.txt\",dtype='str',delimiter=\",\",skiprows=5)\n",
        "    b = data[:,1:17]\n",
        "    floatArray = b.astype(float)\n",
        "    d2 = np.empty((floatArray.shape[0],27*16))\n",
        "    d2 = preprocessing(floatArray)\n",
        "    ts = d2[125*60-1:125*150,:]\n",
        "    label = labeler(ts)\n",
        "    train_data, test_data, train_label, test_label = train_test_split(ts, label, test_size=0.2)\n",
        "    train_x = torch.Tensor(train_data)\n",
        "    test_x = torch.Tensor(test_data)\n",
        "    train_y = torch.LongTensor(train_label)  # torch.int64のデータ型に\n",
        "    test_y = torch.LongTensor(test_label)\n",
        "    train_dataset = TensorDataset(train_x, train_y)\n",
        "    test_dataset = TensorDataset(test_x, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "n_input = 1\n",
        "n_hidden = 10\n",
        "n_output = 1\n",
        "num_layers = 2\n",
        "n_batch = 20\n",
        "n_data = 1000\n",
        "n_test = 200\n"
      ],
      "metadata": {
        "id": "JXWLXvRJOROW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9SBcikGYoX8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class RNNHardCell(nn.Module):\n",
        "    def __init__(self, n_input:int, n_hidden:int, state=None) -> None:\n",
        "        super(RNNHardCell, self).__init__()\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.in_h = nn.Linear(self.n_input, self.n_hidden, bias=False)\n",
        "        self.h_h = nn.Linear(self.n_hidden, self.n_hidden, bias=False)\n",
        "        self.state = state\n",
        "        self.register_parameter()\n",
        "\n",
        "    def register_parameter(self) -> None:\n",
        "        stdv = 1.0 / math.sqrt(self.n_hidden)\n",
        "        for weight in self.parameters():\n",
        "            nn.init.uniform_(weight, -stdv, stdv)\n",
        "    \n",
        "    def forward(self, x, state=None):\n",
        "        self.state = state\n",
        "        if self.state is None:\n",
        "            self.state = F.hardtanh(self.in_h(x))\n",
        "        else:\n",
        "            self.state = F.hardtanh(self.in_h(x) + self.h_h(self.state))\n",
        "        return self.state\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden, n_output, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = RNNHardCell(n_input, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden, n_output, bias=False)\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "    def forward(self, xs, state=None):\n",
        "        state = None\n",
        "        h_seq = []\n",
        "        \n",
        "        for x in xs:\n",
        "            x = torch.from_numpy(np.asarray(x)).float()\n",
        "            x = x.unsqueeze(0)\n",
        "            for _ in range(self.num_layers):\n",
        "                state = self.rnn(x, state)\n",
        "            h_seq.append(state)\n",
        "        \n",
        "        h_seq = torch.stack(h_seq)\n",
        "        ys = self.out(h_seq)\n",
        "        ys = torch.transpose(ys, 0, 1)\n",
        "\n",
        "        return ys"
      ],
      "metadata": {
        "id": "E3TBZ66nVUX1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "y_test_torch = torch.from_numpy(np.asarray(test_y))\n",
        "y_test_torch = y_test_torch.unsqueeze(0)\n",
        "model = RNNModel(n_input, n_hidden, n_output, num_layers)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "MSE = nn.MSELoss()\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "if os.path.exists(\"result/loss\") == False:\n",
        "    os.makedirs(\"result/loss\")\n",
        "if os.path.exists(\"result/eval\") == False:\n",
        "        os.makedirs(\"result/eval\")\n",
        "if os.path.exists(\"result/model\") == False:\n",
        "        os.makedirs(\"result/model\")\n",
        "\n",
        "    \n",
        "for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        perm = np.random.permutation(n_data)\n",
        "        sum_loss = 0\n",
        "for i in range(0, n_data, n_batch):\n",
        "            x_batch = train_data[perm[i:i + n_batch]]\n",
        "            ant_batch = train_label[perm[i:i + n_batch]]\n",
        "            ant_batch = torch.from_numpy(ant_batch).double()\n",
        "            ant_batch = ant_batch.unsqueeze(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_batch_pred = model(x_batch, ant_batch).double()\n",
        "            loss = MSE(y_batch_pred, ant_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.data * n_batch\n",
        "\n",
        "        # loss(train)\n",
        "ave_loss = sum_loss / n_data\n",
        "train_loss.append(ave_loss)\n",
        "\n",
        "        # loss(test)\n",
        "model.eval()\n",
        "y_test_pred = model.forward(test_x)\n",
        "loss = MSE(y_test_pred, y_test_torch)\n",
        "test_loss.append(loss.data)\n",
        "\n",
        "        # loss display\n",
        "if epoch % 100 == 1:\n",
        "            print(\"Ep/MaxEp     train_loss     test_loss\")\n",
        "\n",
        "if epoch % 10 == 0:\n",
        "            print(\"{:4}/{}  {:10.5}   {:10.5}\".format(epoch, epochs, ave_loss, float(loss.data)))\n",
        "\n",
        "if epoch % 20 == 0:\n",
        "            plt.figure(figsize=(5, 4))\n",
        "            y_pred = model.forward(test_x)\n",
        "            # tensor → numpy\n",
        "            y_pred = y_pred.to('cpu').detach().numpy().copy()\n",
        "            plt.plot(test_x, test_y, label = \"target\")\n",
        "            plt.plot(test_x, y_pred[0], label = \"predict\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(0, 2 * np.pi)\n",
        "            plt.ylim(-1.2, 1.2)\n",
        "            plt.xlabel(\"x\")\n",
        "            plt.ylabel(\"y\")\n",
        "            plt.savefig(\"result/eval/ep{}.png\".format(epoch))\n",
        "            plt.clf()\n",
        "            plt.close()\n",
        "\n",
        "    # save loss glaph\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.plot(train_loss, label = \"training\")\n",
        "plt.plot(test_loss, label = \"test\")\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss (MSE)\")\n",
        "plt.savefig(\"result/loss/loss_history.png\")\n",
        "plt.clf()\n",
        "plt.close()\n",
        "\n",
        "    # save best_model\n",
        "torch.save(model, \"result/model/best_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "MvHxWDJuOaga",
        "outputId": "70e56780-3142-4c89-a230-75efca299aa9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d1f2a2dd9b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mant_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mant_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8c1d6ed632d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, state)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mh_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8c1d6ed632d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x432 and 1x10)"
          ]
        }
      ]
    }
  ]
}